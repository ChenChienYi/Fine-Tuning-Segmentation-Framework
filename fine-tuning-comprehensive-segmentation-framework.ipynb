{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"gpuType":"T4","provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":6874929,"sourceType":"datasetVersion","datasetId":3950527}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Background**\n\nThis notebook presents a comprehensive framework for semantic segmentation tasks, with a focus on fine-tuning models on the ISPRS dataset. The core structure is organized into *functional and class-based modules* for data management and model training, followed by the main training script in the final section. You may *skip the first two sections and go directly to the main training script to understand the overall training process*. If you need further details, feel free to refer back to the corresponding functions or classes as needed.\n\n**Overview of Code Structure**\n1. Data Management: Reads image files and their corresponding labels using `read_isprs_images`. Converts RGB labels to class IDs and performs resizing/normalization using the `ISPRSDataset` class.\n\n2. Model Training Workflow:\nSupports both DeepLabv3-ResNet and FCN-ResNet architectures for fine-tuning. All pretrained layers are frozen, while the classifier and auxiliary classifier layers are unfrozen (`initialize_fcn_resnet101_model` and `initialize_deeplabv3_resnet101_model`). Training and validation per epoch are handled by `train_one_epoch` and `test_model`, respectively. The `train_val` function manages hyperparameter variations and logs results using TensorBoard. This project uses intersection over union (IoU) rather than accuracy rate to handle class imbalance in the ISPRS dataset.\n\n3. Main Training:\nIntegrates the data management and training modules, executing a full training pipeline with hyperparameter tuning. After training, prediction results on the test set are visualized using the best-performing model configuration.\n\n**Result**\n\n","metadata":{}},{"cell_type":"code","source":"!pip install rasterio","metadata":{"id":"ubox-SJB2QRm","outputId":"d47f2d8e-a788-4f57-a5d9-798473ec0531","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:26:32.125149Z","iopub.execute_input":"2025-07-19T20:26:32.125323Z","iopub.status.idle":"2025-07-19T20:26:39.040543Z","shell.execute_reply.started":"2025-07-19T20:26:32.125306Z","shell.execute_reply":"2025-07-19T20:26:39.039698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data prepared\nimport warnings\nimport rasterio\nfrom rasterio.errors import NotGeoreferencedWarning\nimport kagglehub\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwarnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)\n\n# data augmentation\nimport cv2\nfrom torchvision import transforms\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# model\nfrom torchvision.models.segmentation import fcn_resnet101, FCN_ResNet101_Weights\nfrom torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nimport itertools\nimport random\nimport torch.optim.lr_scheduler as lr_scheduler\n","metadata":{"id":"IoP7nCT02R9T","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:26:44.350492Z","iopub.execute_input":"2025-07-19T20:26:44.350755Z","iopub.status.idle":"2025-07-19T20:27:15.532863Z","shell.execute_reply.started":"2025-07-19T20:26:44.350725Z","shell.execute_reply":"2025-07-19T20:27:15.532289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Data Management","metadata":{"id":"HXxjXYY7eyZS"}},{"cell_type":"markdown","source":"## 1.1 Loading Images","metadata":{"id":"ExsQnsb_DMPl"}},{"cell_type":"code","source":"def read_isprs_images(image_path, label_path):\n  img_file_list = os.listdir(image_path)\n  lab_file_list = os.listdir(label_path)\n\n  img_file_list = [image_path+'/'+i for i in img_file_list]\n  img_file_list.sort()\n  lab_file_list = [label_path+'/'+i for i in lab_file_list]\n  lab_file_list.sort()\n\n  img, lab = [], []\n  for img_file, lab_file in zip(img_file_list, lab_file_list):\n    with rasterio.open(img_file) as src:\n      img.append(torch.tensor(src.read(), dtype=torch.float32))\n\n    with rasterio.open(lab_file) as src:\n      lab.append(torch.tensor(src.read(), dtype=torch.long))\n\n  return img, lab","metadata":{"id":"5L0dabOKDrdK","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:27:15.534131Z","iopub.execute_input":"2025-07-19T20:27:15.534712Z","iopub.status.idle":"2025-07-19T20:27:15.539675Z","shell.execute_reply.started":"2025-07-19T20:27:15.534693Z","shell.execute_reply":"2025-07-19T20:27:15.539065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ISPRSDataset(torch.utils.data.Dataset):\n  \"\"\"For loading the ISPRS dataset\"\"\"\n  def __init__(self, images, labels):\n    self.img_transform = transforms.Compose([\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\n    self.mask_transform = transforms.Compose([\n          transforms.Resize((300, 300), interpolation=Image.NEAREST)])\n\n    self.images = images\n    self.labels = labels\n\n  def __getitem__(self, idx):\n    image = self.images[idx]\n    label = self.labels[idx]\n\n    if self.img_transform:\n      image = self.img_transform(image)\n      if image.ndim == 2:\n          image = image.unsqueeze(0).repeat(3, 1, 1)\n\n    if self.mask_transform:\n      label_pil_for_resize = Image.fromarray(label.cpu().numpy().astype(np.uint8), mode='L')\n      label_pil_resized = self.mask_transform(label_pil_for_resize)\n      label = torch.from_numpy(np.array(label_pil_resized)).long()\n    return image, label.long()\n\n  def __len__(self):\n    return len(self.images)","metadata":{"id":"F-qkxPCXtkhb","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:27:15.540261Z","iopub.execute_input":"2025-07-19T20:27:15.540497Z","iopub.status.idle":"2025-07-19T20:27:15.584777Z","shell.execute_reply.started":"2025-07-19T20:27:15.540480Z","shell.execute_reply":"2025-07-19T20:27:15.584087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def voc_colormap2label():\n  \"\"\"Build a mapping from RGB to VOC category index (labels)\"\"\"\n  colormap2label = torch.zeros(256 ** 3, dtype=torch.long)\n  for i, colormap in enumerate(ISPRS_COLORMAP):\n      colormap2label[\n          (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n  return colormap2label\n\n# Colormap is the RGB value in the image, which is converted into the corresponding label value\ndef voc_label_indices(colormap, colormap2label):\n  \"\"\"Map RGB values in VOC labels to their category indices\"\"\"\n  colormap = colormap.permute(1, 2, 0).numpy().astype('int32')\n  idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n          + colormap[:, :, 2])\n  return colormap2label[idx]","metadata":{"id":"rQcnEd5Jvbij","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:27:15.586269Z","iopub.execute_input":"2025-07-19T20:27:15.586464Z","iopub.status.idle":"2025-07-19T20:27:15.607549Z","shell.execute_reply.started":"2025-07-19T20:27:15.586448Z","shell.execute_reply":"2025-07-19T20:27:15.606902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_image_and_label(images_list, labels_list, index_to_display = None, is_class_id_label=False):\n  \"\"\"\n  Displays an image and its corresponding label mask from lists of PyTorch tensors.\n  \"\"\"\n\n  if not (0 <= index_to_display < len(images_list)):\n    print(f\"Error: Index {index_to_display} is out of bounds for images_list (size {len(images_list)}).\")\n    return\n  if not (0 <= index_to_display < len(labels_list)):\n    print(f\"Error: Index {index_to_display} is out of bounds for labels_list (size {len(labels_list)}).\")\n    return\n\n  img_tensor = images_list[index_to_display]\n  label_tensor = labels_list[index_to_display]\n\n  img_np = img_tensor.numpy()\n  img_np = np.transpose(img_np, (1, 2, 0))\n\n  if not is_class_id_label:\n    label_indices_np = voc_label_indices(label_tensor.cpu(), voc_colormap2label()).numpy()\n    label_np_processed = label_indices_np\n  else:\n    label_np_processed = label_tensor.numpy()\n\n  def normalize_to_0_1(img_np):\n      min_val = np.min(img_np)\n      max_val = np.max(img_np)\n      if max_val > min_val:\n          img_np_scaled = (img_np - min_val) / (max_val - min_val)\n      else:\n          img_np_scaled = np.zeros_like(img_np)\n\n      return img_np_scaled\n\n  img_np_normalized = normalize_to_0_1(img_np)\n  def mask_to_rgb(mask, colormap):\n      \"\"\"Converts a class index mask (H, W) to an RGB image (H, W, 3) using a colormap.\"\"\"\n      if mask.ndim != 2:\n          raise ValueError(\"Input mask to mask_to_rgb must be 2D (H, W)\")\n\n      h, w = mask.shape\n      output_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n      if 'ISPRS_COLORMAP' in globals():\n            colormap_to_use = ISPRS_COLORMAP\n      else:\n            print(\"Warning: ISPRS_COLORMAP not found. Using a default grayscale colormap.\")\n            colormap_to_use = [[i, i, i] for i in range(len(np.unique(mask)))]\n\n      for class_id, color in enumerate(colormap_to_use):\n          output_rgb[mask == class_id] = color\n      return output_rgb\n\n  label_display_rgb = mask_to_rgb(label_np_processed, ISPRS_COLORMAP)\n\n  plt.figure(figsize=(12, 6))\n  plt.subplot(1, 2, 1)\n  plt.imshow(img_np_normalized)\n  plt.title(f\"Image (Index: {index_to_display})\")\n  plt.axis('off')\n\n  plt.subplot(1, 2, 2)\n  plt.imshow(label_display_rgb)\n  plt.title(f\"Label (Index: {index_to_display})\")\n  plt.axis('off')\n\n  plt.tight_layout()\n  plt.show()","metadata":{"id":"bNOp4EXbDMP5","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:27:15.608325Z","iopub.execute_input":"2025-07-19T20:27:15.608534Z","iopub.status.idle":"2025-07-19T20:27:15.626425Z","shell.execute_reply.started":"2025-07-19T20:27:15.608509Z","shell.execute_reply":"2025-07-19T20:27:15.625936Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Model Training Workflow","metadata":{"id":"JTTmVofT2tV0"}},{"cell_type":"markdown","source":"## 2.1 Model Architecture: FCN_ResNet101","metadata":{"id":"4IxoytWOel0j"}},{"cell_type":"code","source":"def initialize_fcn_resnet101_model(num_output_classes, device):\n  weights = FCN_ResNet101_Weights.DEFAULT\n  model_fcn = fcn_resnet101(weights=weights, progress=False)\n  model_fcn = model_fcn.to(device)\n\n  in_features = model_fcn.classifier[4].in_channels\n\n  model_fcn.classifier[4] = nn.Conv2d(in_features, num_output_classes, kernel_size=(1, 1), stride=(1, 1))\n\n  if model_fcn.aux_classifier is not None:\n    aux_in_features = model_fcn.aux_classifier[4].in_channels\n    model_fcn.aux_classifier[4] = nn.Conv2d(aux_in_features, num_output_classes, kernel_size=(1, 1), stride=(1, 1))\n\n  # freeze all parameters\n  for param in model_fcn.parameters():\n    param.requires_grad = False\n\n  # Unfreeze the parameters of the classifier head\n  for param in model_fcn.classifier.parameters():\n    param.requires_grad = True\n\n  # Unfreeze the parameters of the auxiliary classifier\n  if model_fcn.aux_classifier is not None:\n    for param in model_fcn.aux_classifier.parameters():\n        param.requires_grad = True\n\n  return model_fcn","metadata":{"id":"cCq8fGxvgGD7","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:27:15.627126Z","iopub.execute_input":"2025-07-19T20:27:15.627378Z","iopub.status.idle":"2025-07-19T20:27:15.648314Z","shell.execute_reply.started":"2025-07-19T20:27:15.627356Z","shell.execute_reply":"2025-07-19T20:27:15.647795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2 Model Architecture: deeplabv3_resnet101_model","metadata":{}},{"cell_type":"code","source":"\ndef initialize_deeplabv3_resnet101_model(num_output_classes, device):\n    \"\"\"\n    Initializes and configures the DeepLabV3_ResNet101 model for semantic segmentation.\n    \"\"\"\n    weights = DeepLabV3_ResNet101_Weights.DEFAULT\n    model = deeplabv3_resnet101(weights=weights, progress=False)\n    model = model.to(device)\n\n    in_features_main = model.classifier[4].in_channels\n    model.classifier[4] = nn.Conv2d(in_features_main, num_output_classes, kernel_size=(1, 1), stride=(1, 1))\n\n    if model.aux_classifier is not None:\n        aux_in_features = model.aux_classifier[4].in_channels\n        model.aux_classifier[4] = nn.Conv2d(aux_in_features, num_output_classes, kernel_size=(1, 1), stride=(1, 1))\n    else:\n        pass\n\n    for param in model.parameters():\n        param.requires_grad = False\n\n    for param in model.classifier.parameters():\n        param.requires_grad = True\n\n    if model.aux_classifier is not None:\n        for param in model.aux_classifier.parameters():\n            param.requires_grad = True\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:27:15.648920Z","iopub.execute_input":"2025-07-19T20:27:15.649112Z","iopub.status.idle":"2025-07-19T20:27:15.672510Z","shell.execute_reply.started":"2025-07-19T20:27:15.649094Z","shell.execute_reply":"2025-07-19T20:27:15.672017Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3 Training and Validation Logic","metadata":{"id":"saBwsVVkepVv"}},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, optimizer, loss_fn, device):\n  \"\"\"\n  Run a full training cycle (epoch)\n  \"\"\"\n  model.train()\n  running_train_loss = 0.0\n\n  for i, (features, labels) in enumerate(train_loader):\n    features, labels = features.to(device), labels.to(device)\n    optimizer.zero_grad()\n    classes_preds = model(features)['out']\n    training_loss = loss_fn(classes_preds, labels)\n    training_loss.backward()\n    optimizer.step()\n    running_train_loss += training_loss.item()\n\n  avg_train_loss = running_train_loss / len(train_loader)\n  return model, avg_train_loss","metadata":{"id":"9HXPXuafZtX-","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:27:15.673084Z","iopub.execute_input":"2025-07-19T20:27:15.673259Z","iopub.status.idle":"2025-07-19T20:27:15.695041Z","shell.execute_reply.started":"2025-07-19T20:27:15.673244Z","shell.execute_reply":"2025-07-19T20:27:15.694540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(model, val_loader, loss_fn, device, num_classes, ignore_index):\n  \"\"\"\n  Perform a full validation cycle (epoch)\n  \"\"\"\n  model.eval()\n  running_val_loss = 0.0\n  running_val_iou = 0.0\n  num_batches_with_valid_iou = 0\n\n  with torch.no_grad():\n    for j, (features, labels) in enumerate(val_loader):\n      features, labels = features.to(device), labels.to(device)\n\n      classes_preds = model(features)['out']\n      val_loss = loss_fn(classes_preds, labels)\n      predicted_labels = torch.argmax(classes_preds, dim=1)\n      batch_iou, _ = calculate_iou_mask(\n          predicted_labels.cpu().numpy(),\n          labels.cpu().numpy(),\n          num_classes=num_classes,\n          ignore_index=ignore_index\n      )\n\n      running_val_loss += val_loss.item()\n      if not np.isnan(batch_iou):\n        running_val_iou += batch_iou\n        num_batches_with_valid_iou += 1\n\n  avg_val_loss = running_val_loss / len(val_loader)\n  avg_val_iou = running_val_iou / num_batches_with_valid_iou if num_batches_with_valid_iou > 0 else np.nan\n\n  return avg_val_loss, avg_val_iou","metadata":{"id":"jySXIBfxbOCv","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:32:41.380818Z","iopub.execute_input":"2025-07-19T20:32:41.381121Z","iopub.status.idle":"2025-07-19T20:32:41.386693Z","shell.execute_reply.started":"2025-07-19T20:32:41.381102Z","shell.execute_reply":"2025-07-19T20:32:41.386143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_val(model, hparams, writer):\n  # Device configuration\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  print(f\"Using device: {device}\")\n\n  # hypar\n  lr = hparams['learning_rate']\n  bs = hparams['batch_size']\n  opt_name = hparams['optimizer']\n  gamma = hparams['gamma']\n\n  model = model.to(device)\n  loss_fn = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n\n  if opt_name == 'Adam':\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n  elif opt_name == 'SGD':\n    optimizer = optim.SGD(model.parameters(), lr=lr)\n  else:\n    raise ValueError(f\"Unknown optimizer: {opt_name}\")\n      \n  scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n  \n  # dataset\n  train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, worker_init_fn=worker_init_fn)\n  val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False, worker_init_fn=worker_init_fn)\n\n  # Early stopping\n  best_val_loss = float('inf')\n  patience_counter = 0\n  patience = 2\n\n  n_epoch = 20\n  print(f\"\\nStarting training ...\")\n  for epoch in range(n_epoch):\n    # --- Training Phase ---\n    model, avg_train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n\n    # --- Validation Phase ---\n    avg_val_loss,avg_val_iou = test_model(model, val_loader, loss_fn,\n                                                device, num_classes=NUM_CLASSES, ignore_index=IGNORE_INDEX)\n\n    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n    writer.add_scalar('Metrics/Validation_IoU', avg_val_iou, epoch)\n\n    if epoch % 1 == 0:\n      print(f\"Epoch {epoch+1}/{n_epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val IoU: {avg_val_iou:.4f}\")\n      \n    # --- early stop ---\n    if avg_val_loss < best_val_loss:\n      best_val_loss = avg_val_loss\n      patience_counter = 0\n    else:\n      patience_counter += 1\n      if patience_counter >= patience:\n        print(f\"Early stopping at epoch {epoch+1}\")\n        break\n  \n  # \n    scheduler.step()\n\n  final_loss = avg_val_loss\n  final_iou = avg_val_iou\n  print(\"\\nTraining complete.\")\n  return model, final_loss, avg_val_iou","metadata":{"id":"x9uzZV4cO_F5","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:55:58.853036Z","iopub.execute_input":"2025-07-19T20:55:58.853349Z","iopub.status.idle":"2025-07-19T20:55:58.862063Z","shell.execute_reply.started":"2025-07-19T20:55:58.853327Z","shell.execute_reply":"2025-07-19T20:55:58.861355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_iou_mask(pred_mask, true_mask, num_classes, ignore_index):\n  \"\"\"\n  Calculates Intersection over Union (IoU) for segmentation masks.\n  Can handle binary or multi-class masks.\n  \"\"\"\n  if pred_mask.shape != true_mask.shape:\n    raise ValueError(\"Predicted mask and true mask must have the same shape.\")\n\n  # Ensure masks are integer types\n  pred_mask = pred_mask.astype(np.int64)\n  true_mask = true_mask.astype(np.int64)\n\n  # Flatten masks for easier comparison\n  pred_flat = pred_mask.flatten()\n  true_flat = true_mask.flatten()\n\n  if num_classes is None:\n    all_classes = np.unique(np.concatenate((pred_flat, true_flat)))\n    if ignore_index is not None:\n        all_classes = all_classes[all_classes != ignore_index]\n    num_classes = int(np.max(all_classes)) + 1 if len(all_classes) > 0 else 0\n    if num_classes == 0:\n        return 0.0 if ignore_index is None else np.nan\n\n  iou_per_class = {}\n  total_iou = 0.0\n  valid_classes_count = 0\n\n  for class_id in range(num_classes):\n    if class_id == ignore_index:\n      continue\n\n    # Create binary masks for the current class\n    pred_binary = (pred_flat == class_id)\n    true_binary = (true_flat == class_id)\n\n    intersection = np.sum(pred_binary & true_binary)\n    union = np.sum(pred_binary | true_binary)\n\n    if union == 0:\n      iou_score = np.nan\n    else:\n      iou_score = intersection / union\n\n    iou_per_class[class_id] = iou_score\n\n    if not np.isnan(iou_score):\n      total_iou += iou_score\n      valid_classes_count += 1\n\n  mean_iou = total_iou / valid_classes_count if valid_classes_count > 0 else np.nan\n\n  return mean_iou, iou_per_class","metadata":{"id":"ulCcSf1HOetS","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:32:46.740562Z","iopub.execute_input":"2025-07-19T20:32:46.740826Z","iopub.status.idle":"2025-07-19T20:32:46.747416Z","shell.execute_reply.started":"2025-07-19T20:32:46.740805Z","shell.execute_reply":"2025-07-19T20:32:46.746831Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Main Training Script","metadata":{"id":"APu3pxoGDMQD"}},{"cell_type":"markdown","source":"## 3.1 Data Management","metadata":{"id":"KX7p2lEdDMQD"}},{"cell_type":"code","source":"ISPRS_COLORMAP = [[255, 255, 255], [0, 0, 255], [0, 255, 255], [0, 255, 0],\n                [255, 255, 0], [255, 0, 0]]\n\nISPRS_CLASSES = ['Impervious Surfaces', 'Building', 'Low Vegetation', 'Tree',\n               'Car','Clutter/Background']","metadata":{"id":"Iv1jhQQGDMQE","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:27:15.906626Z","iopub.execute_input":"2025-07-19T20:27:15.907097Z","iopub.status.idle":"2025-07-19T20:27:15.910455Z","shell.execute_reply.started":"2025-07-19T20:27:15.907078Z","shell.execute_reply":"2025-07-19T20:27:15.909935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 0.Device configuration\ndevice = torch.device('cuda')\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n\nprint(f'Use: {device}')\n\n# 1. loading data\npath = kagglehub.dataset_download(\"jahidhasan66/isprs-potsdam\")\npath = path+'/patches'\nprint(\"Path to dataset files:\", path)\nimage_path = path+'/Images'\nlabel_path = path+'/Labels'\nimages, labels = read_isprs_images(image_path, label_path)\n\ncolormap2label = voc_colormap2label()\nlabels_class_ids = [voc_label_indices(label, colormap2label) for label in labels]\nprint(f\"Converted {len(labels)} RGB labels to class ID labels (H, W).\")\n\n# 2. split data\ntrain_ratio = 0.70\nval_ratio = 0.15\ntest_ratio = 0.15\n\ntotal_samples = min(len(images), len(labels_class_ids))\ntrain_count = int(total_samples * train_ratio)\nval_count = int(total_samples * val_ratio)\ntest_count = total_samples - train_count - val_count\n\ntrain_images = images[:train_count]\ntrain_labels = labels_class_ids[:train_count]\n\nval_images = images[train_count : train_count + val_count]\nval_labels = labels_class_ids[train_count : train_count + val_count]\n\ntest_images = images[train_count + val_count : total_samples]\ntest_labels = labels_class_ids[train_count + val_count : total_samples]\n\nprint(f\"Final train_images count: {len(train_images)}, train_labels count: {len(train_labels)}\")\nprint(f\"Final val_images count: {len(val_images)}, val_labels count: {len(val_labels)}\")\nprint(f\"Final test_images count: {len(test_images)}, test_labels count: {len(test_labels)}\")\n\n# v","metadata":{"id":"23xVZgU1q6WQ","outputId":"f515f869-1465-4897-c2c6-f3de0b863dd6","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:27:18.566089Z","iopub.execute_input":"2025-07-19T20:27:18.566346Z","iopub.status.idle":"2025-07-19T20:30:06.716299Z","shell.execute_reply.started":"2025-07-19T20:27:18.566328Z","shell.execute_reply":"2025-07-19T20:30:06.715476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cus dataset\ntrain_dataset = ISPRSDataset(train_images, train_labels)\nval_dataset = ISPRSDataset(val_images, val_labels)\ntest_dataset = ISPRSDataset(test_images, test_labels)\n\nbatch_size = 12\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# # of data, # of batches, check input and output shape\nprint(f\"Train images length: {len(train_images)}, Train labels length: {len(train_labels)}\")\nprint(f\"Validation images length: {len(val_images)}, Validation labels length: {len(val_labels)}\")\nprint(f\"Test images length: {len(test_images)}, Test labels length: {len(test_labels)}\")\n\nprint(f\"Number of batches in train_loader: {len(train_loader)}\")\n\nfor X, Y in train_loader:\n  print(X.shape)\n  print(Y.shape)\n  break\n\n# Visualizate image and mask\ndisplay_example_index = 337\nprint('visualizate a example')\ndisplay_image_and_label(images, labels, display_example_index, is_class_id_label=False)","metadata":{"id":"da14c916","outputId":"185875e0-f23e-4dda-d8c4-5820231fe1f9","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:32:51.391494Z","iopub.execute_input":"2025-07-19T20:32:51.392118Z","iopub.status.idle":"2025-07-19T20:32:51.874064Z","shell.execute_reply.started":"2025-07-19T20:32:51.392093Z","shell.execute_reply":"2025-07-19T20:32:51.873394Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Model Training","metadata":{"id":"Nng5-akgDMQH"}},{"cell_type":"code","source":"# reproducible\nSEED = 77\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n  torch.cuda.manual_seed_all(SEED)\n  torch.backends.cudnn.deterministic = True\n  torch.backends.cudnn.benchmark = False\n\ndef worker_init_fn(worker_id):\n  seed = SEED + worker_id\n  torch.manual_seed(seed)\n  np.random.seed(seed)\n  random.seed(seed)\n  if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# file\nbase_log_dir = \"runs/hparam_search_results\"\nos.makedirs(base_log_dir, exist_ok=True)\n\n# no. of classes, ignore_index\nNUM_CLASSES = len(ISPRS_CLASSES)\nIGNORE_INDEX = 255\n\n# hyperparameters\nlearning_rates = [0.005,0.001]\nbatch_sizes = [32, 64]\noptimizers = ['Adam']\ngammas = [0.90]\n\nMODELS_TO_TEST = [\n    (\"FCN_ResNet101\", initialize_fcn_resnet101_model(6,device)),\n    (\"DeepLabV3_ResNet101\", initialize_deeplabv3_resnet101_model(6,device))\n]\n\nHPARAM_COMBINATIONS = list(itertools.product(learning_rates, batch_sizes, optimizers,gammas))\n\n# training\nprint(\"\\nStarting hyperparameter search across multiple models...\")\n\nfor model_name, model_init_func in MODELS_TO_TEST:\n    print(f\"\\n--- Starting Hyperparameter Search for Model: {model_name} ---\")\n    for i, (lr, bs, opt_name, gamma) in enumerate(HPARAM_COMBINATIONS):\n      hparams = {\n          'learning_rate': lr,\n          'batch_size': bs,\n          'optimizer': opt_name,\n          'model_name': model_name,\n          'gamma':gammas\n      }\n    \n      run_name = f\"{model_name}_run_{i}_lr{lr}_bs{bs}_opt{opt_name}\"\n      log_path = os.path.join(base_log_dir, run_name)\n      writer = SummaryWriter(log_dir=log_path)\n    \n      print(f\"\\n--- Run {i+1}/{len(HPARAM_COMBINATIONS)}: {hparams} ---\")\n      current_model = model_init_func\n      _, final_loss, final_iou = train_val(current_model, hparams, writer)\n    \n      writer.add_hparams(\n          hparam_dict=hparams,\n          metric_dict={\n              'hparam/final_loss': final_loss,\n              'hparam/final_iou': final_iou\n          }\n      )\n    \n      writer.close()\n      print(f\"Experiment {i+1} for {model_name} completed. Final Validation Loss: {final_loss:.4f}, Final Validation IoU: {final_iou:.4f}\")\n\nprint(f\"\\n All data have been recorded: {base_log_dir}\")\n\n\n%load_ext tensorboard\n%tensorboard --logdir runs/hparam_search_results\n","metadata":{"id":"y0XK2PdWBn1G","outputId":"4da99f86-d1d7-4a2a-9000-d80e3cd346fd","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:56:05.602161Z","iopub.execute_input":"2025-07-19T20:56:05.602469Z","iopub.status.idle":"2025-07-19T22:43:41.450860Z","shell.execute_reply.started":"2025-07-19T20:56:05.602445Z","shell.execute_reply":"2025-07-19T22:43:41.450143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# best model\nbest_hparams = {'learning_rate': 0.001, 'batch_size': 32, 'optimizer': 'Adam', 'model_name': 'DeepLabV3_ResNet101', 'gamma': 0.9}\n\nmodel_fcn = initialize_deeplabv3_resnet101_model(6,device)\nmodel, best_final_loss, beast_final_iou = train_val(model_fcn,best_hparams, writer)","metadata":{"id":"Ixzk1im058pZ","outputId":"6cf073ac-163d-4f4e-9d23-4c3b561385cc","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T23:07:07.616995Z","iopub.execute_input":"2025-07-19T23:07:07.617688Z","iopub.status.idle":"2025-07-19T23:26:44.731506Z","shell.execute_reply.started":"2025-07-19T23:07:07.617664Z","shell.execute_reply":"2025-07-19T23:26:44.730646Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Evaluate Model Performance","metadata":{"id":"aIOS2svODMQK"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nloss_fn = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\navg_test_loss, avg_test_iou = test_model(model, test_loader, loss_fn,\n                                         device, num_classes=NUM_CLASSES, ignore_index=IGNORE_INDEX)\n\nprint(f\"Average Test Loss: {avg_test_loss:.4f}\")\nprint(f\"Mean Test IoU: {avg_test_iou:.4f}\")","metadata":{"id":"uAZkl8c5fSvo","outputId":"0425da9e-53d6-43bb-c287-449c7e9a4209","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T23:32:48.873460Z","iopub.execute_input":"2025-07-19T23:32:48.874017Z","iopub.status.idle":"2025-07-19T23:33:16.834254Z","shell.execute_reply.started":"2025-07-19T23:32:48.873991Z","shell.execute_reply":"2025-07-19T23:33:16.833530Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.4 Visualize Prediction Results","metadata":{"id":"994f0ce9"}},{"cell_type":"code","source":"\nnum_visualizations_per_class = 3\ntest_indices = list(range(len(test_dataset)))\nrandom_test_indices = random.sample(test_indices, num_visualizations_per_class)\n\nmodel.eval()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef mask_to_rgb(mask, colormap):\n  \"\"\"Converts a class index mask (H, W) to an RGB image (H, W, 3) using a colormap.\"\"\"\n  h, w = mask.shape\n  output_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n  for class_id, color in enumerate(colormap):\n      output_rgb[mask == class_id] = color\n  return output_rgb\n\nplt.figure(figsize=(15, num_visualizations_per_class * 5))\n\nwith torch.no_grad():\n  for i, idx in enumerate(random_test_indices):\n    image, label = test_dataset[idx]\n    image = image.unsqueeze(0).to(device)\n    prediction = model(image)['out']\n    predicted_mask = torch.argmax(prediction, dim=1).squeeze(0)\n    image = image.squeeze(0).cpu()\n    label = label.cpu()\n    predicted_mask = predicted_mask.cpu()\n\n    ground_truth_rgb = mask_to_rgb(label.numpy(), ISPRS_COLORMAP)\n    predicted_rgb = mask_to_rgb(predicted_mask.numpy(), ISPRS_COLORMAP)\n\n    image_display = image.permute(1, 2, 0).numpy()\n    if image_display.max() > 255 or image_display.min() < 0:\n          image_display = (image_display - image_display.min()) / (image_display.max() - image_display.min()) * 255\n    image_display = image_display.astype(np.uint8)\n\n    plt.subplot(num_visualizations_per_class, 3, i * 3 + 1)\n    plt.imshow(image_display)\n    plt.title(f\"Original Image {idx}\")\n    plt.axis('off')\n\n    plt.subplot(num_visualizations_per_class, 3, i * 3 + 2)\n    plt.imshow(ground_truth_rgb)\n    plt.title(f\"Ground Truth {idx}\")\n    plt.axis('off')\n\n    plt.subplot(num_visualizations_per_class, 3, i * 3 + 3)\n    plt.imshow(predicted_rgb)\n    plt.title(f\"Predicted {idx}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"7d6a0f0e","outputId":"e791b898-ef40-4047-8108-1a1882aea273","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T23:34:36.378033Z","iopub.execute_input":"2025-07-19T23:34:36.378541Z","iopub.status.idle":"2025-07-19T23:34:37.739218Z","shell.execute_reply.started":"2025-07-19T23:34:36.378520Z","shell.execute_reply":"2025-07-19T23:34:37.738375Z"}},"outputs":[],"execution_count":null}]}